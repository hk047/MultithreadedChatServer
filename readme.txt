DICEGAME Readme File

In designing an agent to play the dicegame in an optimal and efficient way, I chose the value iteration algorithm.  The reason why I chose to implement this algorithm is that it has a complexity of O(s^2 * a) for each iteration.  Other algorithms like policy iteration are more complex, requiring a 2 step process that is repeated (policy evaluation, then policy improvement) until is converges. 

The algorithm finds the value of the optimal policy in the dicegame, which is a Markov Decision Process.  It uses the Bellman equation to determine the value of each state in the process.

The algorithm first populates the policy and values dictionaries with dummy values of 0 with keys being each state.  It then continually loops - for every state, checking the actions and calculating the associated values using the Bellman equation.  As the algorithm continues cycling through all the available states, the difference in values found reduces, thus showing the convergence.  The value 'theta' represents the tolerance of the algorithm - the accepted difference when the algorithm considers the values to have converged.  A smaller theta can increase the time taken for the agent to complete its task significantly.  The value 'gamma' considers the algorithm's short/long-term goals.  A higher gamma value will result in the algorithm aiming to maximise long-term gain as oppose to short-term gain.  I set the gamma value to 1 as we do not need to consider the short-term rewards.  